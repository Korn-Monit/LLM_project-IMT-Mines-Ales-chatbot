{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IrpWvpVDteKi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install unsloth langchain chromadb pdfplumber python-dotenv pandas transformers\n",
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install einops\n",
        "!pip install numpy\n",
        "# Import required libraries\n",
        "import torch\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq, pipeline\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
        "from trl import SFTTrainer\n",
        "from langchain.document_loaders import PDFPlumberLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import uuid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQEWmxkLar11"
      },
      "source": [
        "## Load LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy2HEv4yWPDa",
        "outputId": "2d486433-38ca-4eac-f637-eae303e5508e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# 1. First load model and tokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=torch.cuda.is_available(),\n",
        "    # Add flash attention for optimization\n",
        "    attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else None,\n",
        ")\n",
        "\n",
        "# 2. Then apply chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        ")\n",
        "\n",
        "# 3. Prepare for inference\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 4. Verify template application\n",
        "test_messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
        "try:\n",
        "    tokenizer.apply_chat_template(test_messages)\n",
        "except Exception as e:\n",
        "    raise ValueError(\"Chat template application failed!\") from e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE9UhK0PMItX"
      },
      "source": [
        "## vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OavwVKQtJU-f",
        "outputId": "7e1eba24-d237-415f-80ce-5aaa4b726ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install json\n",
        "import json\n",
        "\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "def load_json_faqs(file_paths):\n",
        "    documents = []\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            module_name = file_path.split('_')[-1].split('.')[0].lower()\n",
        "            for item in data:\n",
        "                # Create proper Document objects\n",
        "                content = f\"Q: {item['instruction']}\\nA: {item['output']}\"\n",
        "                metadata = {\n",
        "                    \"source\": f\"FAQ_{module_name}\",\n",
        "                    \"instruction\": item['instruction'],\n",
        "                    \"answer\": item['output']\n",
        "                }\n",
        "\n",
        "                documents.append(Document(\n",
        "                    page_content=content,\n",
        "                    metadata=metadata\n",
        "                ))\n",
        "    return documents\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\nQ:\", \"\\nA:\", \"\\n\\n\"]\n",
        ")\n",
        "\n",
        "\n",
        "file_paths = [\"FAQ_ADVANCED_ML.json\", \"FAQ_DECISION_ANALYSIS.json\",\"FAQ_DL.json\",\"FAQ_MATHS_ML.json\", \"FAQ_MEUH.json\", \"FAQ_PRO_CONTRACT.json\", \"FAQ_SCHOOL.json\", \"GEN_CONVERSATION.json\"]\n",
        "documents = load_json_faqs(file_paths)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\"]\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-olCkMlKCAe"
      },
      "source": [
        "## Embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjQebjj1JgOV",
        "outputId": "b2e6075f-b83b-4a02-b0ff-35efde9ac4b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-aa8e00bfb021>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_function = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error creating vectorstore: \u001b[91mYou are using a deprecated configuration of Chroma.\n",
            "\n",
            "\u001b[94mIf you do not have data you wish to migrate, you only need to change how you construct\n",
            "your Chroma client. Please see the \"New Clients\" section of https://docs.trychroma.com/deployment/migration.\n",
            "________________________________________________________________________________________________\n",
            "\n",
            "If you do have data you wish to migrate, we have a migration tool you can use in order to\n",
            "migrate your data to the new Chroma architecture.\n",
            "Please `pip install chroma-migrate` and run `chroma-migrate` to migrate your data and then\n",
            "change how you construct your Chroma client.\n",
            "\n",
            "See https://docs.trychroma.com/deployment/migration for more information or join our discord at https://discord.gg/MMeYNTmh3x for help!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "embedding_function = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\",\n",
        "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "    encode_kwargs={\n",
        "        \"normalize_embeddings\": True,\n",
        "        \"batch_size\": 32  # Better for multilingual processing\n",
        "    },\n",
        ")\n",
        "\n",
        "# Function to create vectorstore with normalized embeddings\n",
        "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
        "    # Add explicit Chroma client configuration\n",
        "    from chromadb.config import Settings\n",
        "    client_settings = Settings(\n",
        "        chroma_db_impl=\"duckdb+parquet\",\n",
        "        persist_directory=vectorstore_path,\n",
        "        anonymized_telemetry=False,\n",
        "    )\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embedding_function,\n",
        "        client_settings=client_settings,  # Add this line\n",
        "        persist_directory=vectorstore_path,\n",
        "        collection_metadata={\n",
        "            \"hnsw:space\": \"cosine\",\n",
        "            \"dimension\": 1024\n",
        "        },\n",
        "    )\n",
        "    vectorstore.persist()  # Explicitly persist the data\n",
        "    return vectorstore\n",
        "\n",
        "vectorstore_path = \"/content/vectorstore\"  # Use absolute path\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "# Completely remove existing directory with proper permissions\n",
        "if os.path.exists(vectorstore_path):\n",
        "    shutil.rmtree(vectorstore_path, ignore_errors=True)\n",
        "    time.sleep(2)  # Increase wait time\n",
        "\n",
        "\n",
        "# Create fresh directory with write permissions\n",
        "os.makedirs(vectorstore_path, exist_ok=True, mode=0o777)\n",
        "# Set directory permissions explicitly\n",
        "os.chmod(vectorstore_path, 0o777)\n",
        "# Now create the vectorstore\n",
        "try:\n",
        "    vectorstore = create_vectorstore(\n",
        "        chunks=chunks,\n",
        "        embedding_function=embedding_function,\n",
        "        vectorstore_path=vectorstore_path\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error creating vectorstore: {e}\")\n",
        "    # Fallback to in-memory\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embedding_function,\n",
        "        persist_directory=None\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"You are an official advisor at IMT Mines Alès.\n",
        "Answer STRICTLY using only this context: \\n{context}\n",
        "Rules:\n",
        "1. If answer isn't in context, say \"I don't have that information\".\n",
        "2. Keep responses under 3 sentences.\n",
        "3. Never mention you're an AI.\n",
        "\"\"\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k3ZWmrUazQ6"
      },
      "source": [
        "## Hybrid Retrieval (optional for this kind of dataset since the embedding model is strong enough)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o3uszH4spvIt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# Create semantic retriever\n",
        "semantic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "#add re-ranker model\n",
        "reranker_model_name = \"BAAI/bge-reranker-large\"  #if it doesn't work well try bge-reranker-large\n",
        "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
        "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name).to(\"cuda\")\n",
        "\n",
        "#re-rank retreived results\n",
        "def rerank_documents(query, retrieved_docs, top_k=3):\n",
        "    inputs = tokenizer(\n",
        "        [query] * len(retrieved_docs),\n",
        "        [doc.page_content for doc in retrieved_docs],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        scores = reranker_model(**inputs).logits.squeeze(-1)  # Shape: (n_docs,)\n",
        "\n",
        "    ranked_results = sorted(\n",
        "        zip(scores.tolist(), retrieved_docs),\n",
        "        key=lambda x: x[0],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    return [doc for _, doc in ranked_results[:top_k]]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5Hnk3CgPk_V",
        "outputId": "fe08dd2a-5bd9-4ffd-b03c-d73ad3dac5af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "# from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "# !pip install rank_bm25\n",
        "# # Create semantic retriever\n",
        "# semantic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "# # texts = [doc.page_content for doc in chunks]\n",
        "\n",
        "# # Create keyword retriever\n",
        "# # metadatas = [{\"source\": doc.metadata.get(\"source\", \"unknown\")} for doc in chunks]\n",
        "# bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "# bm25_retriever.k = 2\n",
        "\n",
        "# # Combine retrievers\n",
        "# ensemble_retriever = EnsembleRetriever(\n",
        "#     retrievers=[semantic_retriever, bm25_retriever],\n",
        "#     weights=[0.6, 0.4]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c0ysxYXJP5pL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def validate_response(response: str, context: str) -> str:\n",
        "    # First check: Empty response\n",
        "    if not response.strip():\n",
        "        return \"I don't have that information.\"\n",
        "\n",
        "    # Second check: Direct contradiction\n",
        "    validation_pipe = pipeline(\n",
        "        \"zero-shot-classification\",\n",
        "        model=\"facebook/bart-large-mnli\",\n",
        "        device=0 if torch.cuda.is_available() else -1,\n",
        "    )\n",
        "\n",
        "    result = validation_pipe(\n",
        "        sequences=context,\n",
        "        candidate_labels=[\"relevant\", \"irrelevant\"],\n",
        "        hypothesis_template=\"This context supports the response: '{}'\",\n",
        "        multi_label=False,\n",
        "    )\n",
        "\n",
        "    if result['labels'][0] == \"irrelevant\":\n",
        "        return \"I cannot confirm this information.\"\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ud2INST7WwL9"
      },
      "outputs": [],
      "source": [
        "from transformers import StoppingCriteria\n",
        "\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __init__(self, stop_token_ids):\n",
        "        self.stop_token_ids = stop_token_ids\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        return any(token in self.stop_token_ids for token in input_ids[0][-3:])\n",
        "\n",
        "# Add this right after the class definition:\n",
        "stop_token_ids = [tokenizer.eos_token_id] + [\n",
        "    tokenizer.encode(token, add_special_tokens=False)[-1]  # Get last token ID\n",
        "    for token in [\"\\nReferences:\", \"\\nSource:\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fpEXtTRFcdXI"
      },
      "outputs": [],
      "source": [
        "# Add these imports at the top\n",
        "from typing import Dict, List\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "# 1. Conversation State Manager\n",
        "class ChatSession:\n",
        "    def __init__(self):\n",
        "        self.history: List[Dict] = []\n",
        "        self.context_window = 2048  # Match model's max_seq_length\n",
        "\n",
        "    def add_interaction(self, question: str, answer: str):\n",
        "        self.history.extend([\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "            {\"role\": \"assistant\", \"content\": answer}\n",
        "        ])\n",
        "\n",
        "    def get_recent_history(self, token_limit: int = 512) -> str:\n",
        "        \"\"\"Return truncated conversation history\"\"\"\n",
        "        history_text = \"\\n\".join(\n",
        "            f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
        "            for msg in self.history[-4:]  # Last 2 exchanges\n",
        "        )\n",
        "        return self._truncate_text(history_text, token_limit)\n",
        "\n",
        "    def _truncate_text(self, text: str, max_tokens: int) -> str:\n",
        "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "        return tokenizer.decode(tokens[-max_tokens:], skip_special_tokens=True)\n",
        "\n",
        "# 2. Enhanced RAG Pipeline with History\n",
        "def interactive_rag_pipeline(question: str, session: ChatSession) -> str:\n",
        "    # Incorporate conversation history in retrieval\n",
        "    combined_query = f\"{question}\"\n",
        "    # recent_history = session.get_recent_history()\n",
        "    # Retrieve documents with history context\n",
        "    docs = semantic_retriever.invoke(combined_query)\n",
        "    #new code added\n",
        "    reranked_docs = rerank_documents(question, docs)\n",
        "    print(f\"Retrieved \\n{docs}\")\n",
        "    context = \"\\n\".join([d.page_content for d in reranked_docs[:3]])\n",
        "    #new code added\n",
        "    # Print the reranked documents\n",
        "    print(\"\\nTop reranked documents:\\n\")\n",
        "    for i, doc in enumerate(reranked_docs, start=1):\n",
        "        print(f\"--- Document {i} ---\") # <-- This line prints the document number\n",
        "        print(doc.page_content)\n",
        "    print()\n",
        "\n",
        "    # Build messages with history\n",
        "    messages = [\n",
        "        system_message.copy(),\n",
        "        *session.history,\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    messages[0][\"content\"] = messages[0][\"content\"].replace(\"{context}\", context)\n",
        "    print(f\"Messages: {messages}\")\n",
        "    # Generate response\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n",
        "    print(f\"debugging: {prompt}\")\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.5,\n",
        "        stopping_criteria=[StopOnTokens(stop_token_ids)],\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    clean_response = response.split(\"assistant<|end_header_id|>\")[-1].split(\"<|eot_id|>\")[0].strip()\n",
        "\n",
        "    # Validate and format\n",
        "    final_response = validate_response(clean_response, context)\n",
        "    session.add_interaction(question, final_response)\n",
        "\n",
        "    return final_response\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cB5BeGOaHKh"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOyUmyWcVoN3",
        "outputId": "9cba9a5f-c96e-4c89-f037-3362f8fb1a13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Testing interactive_rag_pipeline directly ---\n",
            "Retrieved \n",
            "[Document(metadata={'answer': 'Sébastien HARISPE is the coordinator of the module. He can be contacted via email at sebastien.harispe@mines-ales.fr or phone at +33434246282.', 'instruction': 'Who is the coordinator of deep learning module?', 'source': 'FAQ_dl'}, page_content='Q: Who is the coordinator of deep learning module?\\nA: Sébastien HARISPE is the coordinator of the module. He can be contacted via email at sebastien.harispe@mines-ales.fr or phone at +33434246282.'), Document(metadata={'source': 'FAQ_dl', 'instruction': 'Who oversees the deep learning module?', 'answer': 'The module is managed by Sébastien HARISPE. You can reach him at sebastien.harispe@mines-ales.fr or by calling +33434246282.'}, page_content='Q: Who oversees the deep learning module?\\nA: The module is managed by Sébastien HARISPE. You can reach him at sebastien.harispe@mines-ales.fr or by calling +33434246282.'), Document(metadata={'instruction': 'Who is responsible for deep learning module?', 'answer': 'The module is overseen by Sébastien HARISPE, who can be contacted via email at sebastien.harispe@mines-ales.fr or by phone at +33434246282.', 'source': 'FAQ_dl'}, page_content='Q: Who is responsible for deep learning module?\\nA: The module is overseen by Sébastien HARISPE, who can be contacted via email at sebastien.harispe@mines-ales.fr or by phone at +33434246282.'), Document(metadata={'answer': 'Students should contact Sébastien HARISPE for any queries related to the module. His email is sebastien.harispe@mines-ales.fr, and his phone number is +33434246282.', 'source': 'FAQ_dl', 'instruction': 'Who should students contact regarding the module?'}, page_content='Q: Who should students contact regarding the module?\\nA: Students should contact Sébastien HARISPE for any queries related to the module. His email is sebastien.harispe@mines-ales.fr, and his phone number is +33434246282.'), Document(metadata={'answer': 'The teaching staff includes Sébastien HARISPE, Anne JOHANNET, and Patrice GUYOT, who are experts in Artificial Intelligence, Machine Learning, neural networks, signal processing, and knowledge representation.', 'source': 'FAQ_dl', 'instruction': 'Who teaches the deep learning module?'}, page_content='Q: Who teaches the deep learning module?\\nA: The teaching staff includes Sébastien HARISPE, Anne JOHANNET, and Patrice GUYOT, who are experts in Artificial Intelligence, Machine Learning, neural networks, signal processing, and knowledge representation.')]\n",
            "\n",
            "Top reranked documents:\n",
            "\n",
            "--- Document 1 ---\n",
            "Q: Who oversees the deep learning module?\n",
            "A: The module is managed by Sébastien HARISPE. You can reach him at sebastien.harispe@mines-ales.fr or by calling +33434246282.\n",
            "--- Document 2 ---\n",
            "Q: Who is the coordinator of deep learning module?\n",
            "A: Sébastien HARISPE is the coordinator of the module. He can be contacted via email at sebastien.harispe@mines-ales.fr or phone at +33434246282.\n",
            "--- Document 3 ---\n",
            "Q: Who teaches the deep learning module?\n",
            "A: The teaching staff includes Sébastien HARISPE, Anne JOHANNET, and Patrice GUYOT, who are experts in Artificial Intelligence, Machine Learning, neural networks, signal processing, and knowledge representation.\n",
            "\n",
            "Messages: [{'role': 'system', 'content': 'You are an official advisor at IMT Mines Alès.\\nAnswer STRICTLY using only this context: \\nQ: Who oversees the deep learning module?\\nA: The module is managed by Sébastien HARISPE. You can reach him at sebastien.harispe@mines-ales.fr or by calling +33434246282.\\nQ: Who is the coordinator of deep learning module?\\nA: Sébastien HARISPE is the coordinator of the module. He can be contacted via email at sebastien.harispe@mines-ales.fr or phone at +33434246282.\\nQ: Who teaches the deep learning module?\\nA: The teaching staff includes Sébastien HARISPE, Anne JOHANNET, and Patrice GUYOT, who are experts in Artificial Intelligence, Machine Learning, neural networks, signal processing, and knowledge representation.\\nRules:\\n1. If answer isn\\'t in context, say \"I don\\'t have that information\".\\n2. Keep responses under 3 sentences.\\n3. Never mention you\\'re an AI.\\n'}, {'role': 'user', 'content': 'Who is Sebastien Haripse?'}]\n",
            "debugging: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are an official advisor at IMT Mines Alès.\n",
            "Answer STRICTLY using only this context: \n",
            "Q: Who oversees the deep learning module?\n",
            "A: The module is managed by Sébastien HARISPE. You can reach him at sebastien.harispe@mines-ales.fr or by calling +33434246282.\n",
            "Q: Who is the coordinator of deep learning module?\n",
            "A: Sébastien HARISPE is the coordinator of the module. He can be contacted via email at sebastien.harispe@mines-ales.fr or phone at +33434246282.\n",
            "Q: Who teaches the deep learning module?\n",
            "A: The teaching staff includes Sébastien HARISPE, Anne JOHANNET, and Patrice GUYOT, who are experts in Artificial Intelligence, Machine Learning, neural networks, signal processing, and knowledge representation.\n",
            "Rules:\n",
            "1. If answer isn't in context, say \"I don't have that information\".\n",
            "2. Keep responses under 3 sentences.\n",
            "3. Never mention you're an AI.\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Who is Sebastien Haripse?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "\n",
            "--- Final Response from Pipeline ---\n",
            "Sébastien HARISPE is the one who oversees the deep learning module at IMT Mines Alès. You can reach him at sebastien.harispe@mines-ales.fr or by calling +33434246282.\n",
            "\n",
            "--- History in Test Session ---\n",
            "[{'role': 'user', 'content': 'Who is Sebastien Haripse?'}, {'role': 'assistant', 'content': 'Sébastien HARISPE is the one who oversees the deep learning module at IMT Mines Alès. You can reach him at sebastien.harispe@mines-ales.fr or by calling +33434246282.'}]\n"
          ]
        }
      ],
      "source": [
        "test_session = ChatSession()\n",
        "test_question = \"Who is Sebastien Haripse?\"\n",
        "# Call the pipeline function directly\n",
        "print(\"\\n--- Testing interactive_rag_pipeline directly ---\")\n",
        "response = interactive_rag_pipeline(test_question, test_session)\n",
        "\n",
        "print(\"\\n--- Final Response from Pipeline ---\")\n",
        "print(response)\n",
        "print(\"\\n--- History in Test Session ---\")\n",
        "print(test_session.history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2bx2WhoYLl"
      },
      "source": [
        "## Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pT3ndOxnlFAw",
        "outputId": "f8f7390c-a64f-40dc-e4ce-ab9782abf69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.30.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.9)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.10)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4c04d247cc5c850268.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4c04d247cc5c850268.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install gradio\n",
        "import gradio as gr\n",
        "from typing import List, Tuple\n",
        "def gradio_chat_interface(message: str, history: List[Tuple[str, str]]):\n",
        "    \"\"\"Handles chat interactions\"\"\"\n",
        "    if not hasattr(gr, \"session\"):\n",
        "        gr.session = ChatSession()\n",
        "\n",
        "    if message.lower() == \"/reset\":\n",
        "        gr.session = ChatSession()\n",
        "        return \"History reset!\"\n",
        "\n",
        "    response = interactive_rag_pipeline(message, gr.session)\n",
        "    return response\n",
        "\n",
        "# Simplified ChatInterface without newer parameters\n",
        "demo = gr.ChatInterface(\n",
        "    fn=gradio_chat_interface,\n",
        "    title=\"IMT Mines Ales RAG Assistant\",\n",
        "    description=\"Ask me anything! Type /reset to clear history.\",\n",
        "    examples=[\"Where is IMT Mines Ales?\", \"Explain quantum physics\"],\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq8TkQ6wQEiZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
